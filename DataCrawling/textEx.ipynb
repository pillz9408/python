{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 마이닝\n",
    "    - 비정형 데이터부터 인사이트 추출을 위해 수집된 데이터를 정제하고 범주화\n",
    "    - 텍스트 수집(크롤링,스크래핑) -> 텍스트 정제(정규표현식,텍스트 전처리) -> 텍스트 마이닝(빈도분석,연관어 분석)-> 인사이트 도출 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석\n",
    " - 오늘 나는 친구와 함께 영화를 볼 계획이다 -> 오늘/ 나/ 는/ 친구/ 와 / 함께/ 영화/를 / 볼/계획/이다/.\n",
    " - 필요하다고 생각되어지는 태그만 추출해서 사용한다. -> ('오늘','Noun'),('는','Josa')...('.''Punctuation')\n",
    "### 형태소 분석기\n",
    " - 영어 : NLTK 라이브러리\n",
    " - 한국어 : Konlpy 라이브러리\n",
    "    - Hannanum : 정제된 언어가 아니면 분석 품질 저하, 띄어쓰기 없는 문장 취약\n",
    "    - Okt : 어근화 가능, 다른 분석기에 비해 비정제 언어도 비교적 나쁘지 않은편 (가장 많이 쓰임)\n",
    "    - Komoran: 오탈자 분석 잘되고 로딩시간 길다, 띄어쓰기 없는 문장 취약\n",
    "    - Kkma : 문장이 늘어날수록 시간이 급격히 증가\n",
    "    - mecab : mac에서만 사용 가능 (속도 제일 빠르다..)\n",
    "\n",
    " - 모든 라이브러리가 동일하게 제공하는 함수\n",
    "   - ___.morphs(text) : 형태소 단위로 분리\n",
    "   - ___.nouns(text) : 명사만 추출\n",
    "   - ___.pos(text) : 품사 태깅\n",
    " - okt 만 제공하는 함수\n",
    "   - 문장 정규화 : okt.morphs(text, norm=True)\n",
    "   - 어간 추출 : okt.morphs(text, stem=True)\n",
    "   - 동시사용 : okt.morphs(text, norm=True, stem=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['설치', '잘', '됐겠지', '?']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt= Okt()\n",
    "\n",
    "okt.morphs('설치 잘 됐겠지?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['너', '가', '한나눔이냐', '?']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "hannanum =Hannanum()\n",
    "hannanum.morphs('네가 한나눔이냐?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['코', '모란', '은', '모란', '의', '코', '이', 'ㄴ가', '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "komoran.morphs('코모란은 모란의 코인가.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['꼬꼬마', ',', '코', '코마', '둘', '중', '무엇', '이', 'ㄴ가요', '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "kkma.morphs('꼬꼬마,코코마 둘중 무엇인가요.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '나는 지금 집에 매우매우 가고 싶다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('나', 'Noun'),\n",
       " ('는', 'Josa'),\n",
       " ('지금', 'Noun'),\n",
       " ('집', 'Noun'),\n",
       " ('에', 'Josa'),\n",
       " ('매우', 'Noun'),\n",
       " ('매우', 'Noun'),\n",
       " ('가고', 'Verb'),\n",
       " ('싶다', 'Verb'),\n",
       " ('.', 'Punctuation')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.morphs(text)\n",
    "okt.pos(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = '나는 진짜 지금 집에 너무 가고 싶은뎈ㅋㅋㅋㅋㅋ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('나', 'Noun'),\n",
       " ('는', 'Josa'),\n",
       " ('진짜', 'Noun'),\n",
       " ('지금', 'Noun'),\n",
       " ('집', 'Noun'),\n",
       " ('에', 'Josa'),\n",
       " ('너무', 'Adverb'),\n",
       " ('가고', 'Verb'),\n",
       " ('싶은뎈', 'Noun'),\n",
       " ('ㅋㅋㅋㅋㅋ', 'KoreanParticle')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.pos(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '는', '진짜', '지금', '집', '에', '너무', '가다', '싶은뎈', 'ㅋㅋㅋㅋㅋ']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.morphs(text1, stem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '는', '진짜', '지금', '집', '에', '너무', '가고', '싶은데', 'ㅋㅋㅋ']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.morphs(text1, norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '는', '진짜', '지금', '집', '에', '너무', '가다', '싶다', 'ㅋㅋㅋ']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.morphs(text1, stem=True, norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불용어 제거하기\n",
    "    - 자주 사용되지만 특별한 의미 부여가 어려운 단어들을 제거\n",
    "    - 일반적으로 불용어 사전은 리스트 형태를 만들어 사용\n",
    "\n",
    "### 실습예제\n",
    "    - 불용어 사전이 있다고 가정했을 때 불용어를 제거하는 define 함수를 만들어보자\n",
    "    - 불용어가 제거되고, 형용사와 동사만 추출할 수 있는 define 함수를 만들어보자\n",
    "        -> 태그이름 형용사 : 'Adjective' 동사 : 'Verb' 명사: 'Noun'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nouns(string):\n",
    "    nouns = okt.nouns(string)\n",
    "    return nouns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '진짜', '지금', '집', '싶은뎈']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_nouns(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['우리','나라','만세']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tags(string):\n",
    "    result_list =[]\n",
    "    \n",
    "    result = okt.pos(string, stem=True, norm=True)\n",
    "    for word,tag in result:\n",
    "        if tag in['Verb','Adjective','Noun']:\n",
    "            if word not in stopwords:\n",
    "                result_list.append(word)\n",
    "            result_list.append(word)   \n",
    "    return result_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '동해물과 백두산이 마르고 닳도록.'\n",
    "text1 = '하느님이 보우하사 우리 나라 만세~'\n",
    "text2 = '무궁화 삼천리 화려강산'\n",
    "text3 = '대한사람 대한으로 우리 나라 만세'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['하느님', '하느님', '보우', '보우', '하사', '하사', '우리', '나라', '만세']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_tags(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영어 형태소 분석\n",
    "    - 형태소 분석 : nltk.word_tokenize(text)\n",
    "    - tokenizer로 토큰화된 리스트를 인풋 데이터로 넣어주어야한다.\n",
    "    - split_text = nltk.word_tokenize(text)\n",
    "    - nltk.pos_tag(split_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "### !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Type anywords what you wants blah blah blah Maybe Today class will end early'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Type',\n",
       " 'anywords',\n",
       " 'what',\n",
       " 'you',\n",
       " 'wants',\n",
       " 'blah',\n",
       " 'blah',\n",
       " 'blah',\n",
       " 'Maybe',\n",
       " 'Today',\n",
       " 'class',\n",
       " 'will',\n",
       " 'end',\n",
       " 'early']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_text = nltk.word_tokenize(text)\n",
    "split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Type', 'NN'),\n",
       " ('anywords', 'NNS'),\n",
       " ('what', 'WP'),\n",
       " ('you', 'PRP'),\n",
       " ('wants', 'VBZ'),\n",
       " ('blah', 'JJ'),\n",
       " ('blah', 'NN'),\n",
       " ('blah', 'NN'),\n",
       " ('Maybe', 'RB'),\n",
       " ('Today', 'NNP'),\n",
       " ('class', 'NN'),\n",
       " ('will', 'MD'),\n",
       " ('end', 'VB'),\n",
       " ('early', 'JJ')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(split_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'type anywords what you wants blah blah blah maybe today class will end early'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습예제\n",
    " - 불용어 사전이 있다고 할 때 불용어를 제거할 수 있는 define 함수를 만들어보자\n",
    " - 불용어가 제거되고 형용사만 추출할 수 있는 define 함수를 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entag(string):\n",
    "    result_lists = []\n",
    "    string = nltk.word_tokenize(string)\n",
    "    result = nltk.pos_tag(string)\n",
    "    for word, tag in result:\n",
    "        if tag in ['JJ','JJR','JJS']:\n",
    "            result_lists.append(word)\n",
    "            if word.lower() not in stopwords:\n",
    "                result_lists.append(word)\n",
    "    return result_lists\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blah', 'blah', 'early', 'early']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = extract_entag(text)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 피클 라이브러리 \n",
    "    - 데이터를 데이터프레임의 형태로 저장하기 곤란할때, 데이터를 타입 그대로 저장해주는 파이썬 내장 라이브러리\n",
    "        * import pickle\n",
    "        * 저장하기 : with open('파일이름.pkl',\"wb\") as f:pickle.dump(저장할변수,f)\n",
    "        * 불러오기 : with open('불러올 파일 이름.pkl','rb') as f : 저장변수 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('save.pkl','wb') as f:pickle.dump(a, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('save.pkl', 'rb') as f : data = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blah', 'blah', 'early', 'early']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39a34cc34fa105d8ec3534aa54504572aead214367e412b05dc49f32cf35b4ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
